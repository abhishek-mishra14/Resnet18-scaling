{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nimport math\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:19.530445Z","iopub.execute_input":"2022-04-29T19:04:19.531274Z","iopub.status.idle":"2022-04-29T19:04:19.536191Z","shell.execute_reply.started":"2022-04-29T19:04:19.531220Z","shell.execute_reply":"2022-04-29T19:04:19.535364Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nsns.set()\nstyle=\"darkgrid\"","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:20.640085Z","iopub.execute_input":"2022-04-29T19:04:20.640410Z","iopub.status.idle":"2022-04-29T19:04:20.647442Z","shell.execute_reply.started":"2022-04-29T19:04:20.640371Z","shell.execute_reply":"2022-04-29T19:04:20.646653Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:20.916118Z","iopub.execute_input":"2022-04-29T19:04:20.916879Z","iopub.status.idle":"2022-04-29T19:04:20.921109Z","shell.execute_reply.started":"2022-04-29T19:04:20.916838Z","shell.execute_reply":"2022-04-29T19:04:20.920287Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"    torch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:21.255872Z","iopub.execute_input":"2022-04-29T19:04:21.256479Z","iopub.status.idle":"2022-04-29T19:04:21.265268Z","shell.execute_reply.started":"2022-04-29T19:04:21.256444Z","shell.execute_reply":"2022-04-29T19:04:21.264478Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\nnum_classes = 10","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:21.602277Z","iopub.execute_input":"2022-04-29T19:04:21.602493Z","iopub.status.idle":"2022-04-29T19:04:21.605735Z","shell.execute_reply.started":"2022-04-29T19:04:21.602468Z","shell.execute_reply":"2022-04-29T19:04:21.605075Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"all_transforms = transforms.Compose([\n                                transforms.Resize(32),\n                                transforms.ToTensor(),\n                                transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n])","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:21.879607Z","iopub.execute_input":"2022-04-29T19:04:21.880266Z","iopub.status.idle":"2022-04-29T19:04:21.884561Z","shell.execute_reply.started":"2022-04-29T19:04:21.880230Z","shell.execute_reply":"2022-04-29T19:04:21.883797Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"train_ds = torchvision.datasets.CIFAR10(root='./data', train=True, transform=all_transforms, download='True')\ntest_ds = torchvision.datasets.CIFAR10(root='./data', train=False, transform=all_transforms, download='True')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:22.140507Z","iopub.execute_input":"2022-04-29T19:04:22.140831Z","iopub.status.idle":"2022-04-29T19:04:23.671172Z","shell.execute_reply.started":"2022-04-29T19:04:22.140792Z","shell.execute_reply":"2022-04-29T19:04:23.670404Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"train_ds, val_ds = torch.utils.data.random_split(train_ds, [45000, 5000])","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:23.672824Z","iopub.execute_input":"2022-04-29T19:04:23.673171Z","iopub.status.idle":"2022-04-29T19:04:23.681719Z","shell.execute_reply.started":"2022-04-29T19:04:23.673133Z","shell.execute_reply":"2022-04-29T19:04:23.681054Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n\n\nval_loader = torch.utils.data.DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(val_ds, batch_size=batch_size, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:23.682864Z","iopub.execute_input":"2022-04-29T19:04:23.683249Z","iopub.status.idle":"2022-04-29T19:04:23.690056Z","shell.execute_reply.started":"2022-04-29T19:04:23.683213Z","shell.execute_reply":"2022-04-29T19:04:23.689222Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"class ResBlock(nn.Module):\n    \n    def __init__(self, in_channels, out_channels, identity_downsample=None, stride=1):\n        super(ResBlock, self).__init__()\n        \"\"\"\n        1. kernel_size = 3 and padding = 1 for both conv layers\n        2. use the argument stride for conv1 layer and stride=1 for conv2 layer\n        3. input channels = in_channels and output channels = out_channels for conv1\n        4. input and output channels = out_channels for conv2\n        \"\"\"\n        ### YOUR CODE HERE ###\n        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, dilation=1)\n        self.bn1 = torch.nn.BatchNorm2d(out_channels)\n        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1)\n        self.bn2 = torch.nn.BatchNorm2d(out_channels)\n        self.relu = torch.nn.ReLU()\n        ### ENDS HERE ###\n        \"\"\"\n        We'll need identity_downsample when the dimensions f(x), i.e., output of\n        step2 is not the same as x\n        \"\"\"\n        self.identity_downsample = identity_downsample\n        \n        \n    def forward(self, x):\n        ### YOUR CODE HERE ###\n        #step 1\n        identity = x\n        #step2\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n\n\n        #do not modify this if condition, change your variable names accordingly\n        if self.identity_downsample is not None:\n            identity = self.identity_downsample(identity)\n        #step3\n        x = x + identity\n        #step4\n        x = self.relu(x)\n        \n        ### ENDS HERE ###\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:23.691976Z","iopub.execute_input":"2022-04-29T19:04:23.692237Z","iopub.status.idle":"2022-04-29T19:04:23.704412Z","shell.execute_reply.started":"2022-04-29T19:04:23.692204Z","shell.execute_reply":"2022-04-29T19:04:23.703510Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"class ResNet18(nn.Module):\n    # d is the depth scaling factor, whereas w is the width scaling factor\n    def __init__(self, image_channels, num_classes):\n        phi = 1\n        alpha = 1.1\n        beta = 1.2\n        d,w = self.calculate_scalingFactors(alpha,beta,phi)\n        \n        super(ResNet18, self).__init__()\n        self.in_channels = 64\n        self.conv1 = nn.Conv2d(image_channels, int(64*w) , kernel_size=7, stride=2, padding=3)\n        self.bn1 = nn.BatchNorm2d(int(64*w)) \n        self.relu = nn.ReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n       #Width scaling by a factor means increasing the no of channels i.e. no of filters across each ResBlock\n        self.layer1 = self.make_layer(int(64*w), int(64*w), 1,d)\n        self.layer2 = self.make_layer(int(64*w), int(128*w), 2,d)\n        self.layer3 = self.make_layer(int(128*w), int(256*w), 2,d)\n        self.layer4 = self.make_layer(int(256*w), int(512*w), 2,d)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        #Aladdin uses a last channels variable we directly code it as 512*w\n        self.fc = nn.Linear(int(512*w), num_classes)\n\n        \n    def make_layer(self, in_channels, out_channels, stride, depth_scaling):\n        identity_downsample = None\n        repeats = 2\n        layer_repeat = math.ceil(depth_scaling*repeats)\n        if stride != 1:\n            identity_downsample = self.identity_downsample(in_channels, out_channels)\n\n        \"\"\"\n        Call `ResBlock` here.  You already have info about all the arguments \n        that needs to be passed to ResBlock.\n        Hint1: use nn.Sequential to make one layer.\n        Hint2: you will need two ResBlocks to make one layer\n        \"\"\"\n        '''\n        DOUBT: We need identity downsample even when stride != 1\n        also same downsampler for both resnet blocks? don't they have different input size?\n        also def identity_downsample(self, in_channels, out_channels): does not have stride? (so I just wrote the code directly up above)\n        '''\n        #model =  nn.Sequential(\n        #    ResBlock(in_channels, out_channels, identity_downsample, stride),\n        #    ResBlock(out_channels, out_channels) \n        #)\n        \n#         model = nn.Sequential(ResBlock(in_channels,out_channels,identity_downsample,stride))\n#         for i in range(layer_repeat-1):\n#             model.append(ResBlock(out_channels,out_channels))\n#         return model\n\n        model = nn.Sequential(ResBlock(in_channels,out_channels,identity_downsample,stride))\n        for i in range(layer_repeat-1):\n            nn.Sequential(model, ResBlock(out_channels,out_channels))\n        return model\n#         model = [ResBlock(in_channels,out_channels,identity_downsample,stride)]\n#         for i in range(layer_repeat-1):\n#             model.append(ResBlock(out_channels,out_channels))\n#          return nn.Sequential(*model)   \n        \n\n    def identity_downsample(self, in_channels, out_channels):\n        \n        return nn.Sequential(\n                nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=2,padding=1),\n                nn.BatchNorm2d(out_channels)\n            ) \n    \n    def calculate_scalingFactors(self, alpha,beta,phi):\n        depth_factor = alpha ** phi\n        width_factor = beta ** phi\n        return depth_factor,width_factor\n        \n    def forward(self, x):\n        \n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = self.avgpool(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc(x)\n        return x ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:23.997140Z","iopub.execute_input":"2022-04-29T19:04:23.997362Z","iopub.status.idle":"2022-04-29T19:04:24.015361Z","shell.execute_reply.started":"2022-04-29T19:04:23.997330Z","shell.execute_reply":"2022-04-29T19:04:24.014611Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\nmodel = ResNet18(3, num_classes)\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:24.360187Z","iopub.execute_input":"2022-04-29T19:04:24.360402Z","iopub.status.idle":"2022-04-29T19:04:24.568313Z","shell.execute_reply.started":"2022-04-29T19:04:24.360376Z","shell.execute_reply":"2022-04-29T19:04:24.567524Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"learning_rate = 0.001\nnum_epochs = 100","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:24.623887Z","iopub.execute_input":"2022-04-29T19:04:24.624103Z","iopub.status.idle":"2022-04-29T19:04:24.627685Z","shell.execute_reply.started":"2022-04-29T19:04:24.624078Z","shell.execute_reply":"2022-04-29T19:04:24.626742Z"},"trusted":true},"execution_count":125,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n\n# Set optimizer with optimizer\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n\ntotal_step = len(train_loader)\n\npatience = 5","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:25.002698Z","iopub.execute_input":"2022-04-29T19:04:25.003125Z","iopub.status.idle":"2022-04-29T19:04:25.009097Z","shell.execute_reply.started":"2022-04-29T19:04:25.003090Z","shell.execute_reply":"2022-04-29T19:04:25.007983Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"\ntest_accu = []\ntest_losses = []\n\ncurr_acc = 0\n\ndef validation(model, device, valid_loader, loss_function):\n    # Settings\n    model.eval()\n    loss_total = 0\n    \n    epoch_loss = []\n    running_loss=0\n    correct=0\n    total=0\n    # Test validation data\n    with torch.no_grad():\n        for i, (images, labels) in enumerate(valid_loader):\n            images = images.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss_total += loss.item()\n            \n            \n            running_loss +=loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n        \n    test_loss=running_loss/len(train_loader)\n    accu=100.*correct/total\n    test_accu.append(accu)\n    test_losses.append(train_loss)\n    \n    if accu>curr_acc:\n        joblib.dump(model, 'filename.pkl')\n\n    return loss_total / len(valid_loader)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:25.371780Z","iopub.execute_input":"2022-04-29T19:04:25.372009Z","iopub.status.idle":"2022-04-29T19:04:25.381383Z","shell.execute_reply.started":"2022-04-29T19:04:25.371982Z","shell.execute_reply":"2022-04-29T19:04:25.380397Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"code","source":"torch.manual_seed(0)\n\n\ntrain_accu = []\ntrain_losses = []\n\nthe_last_loss = math.inf\nmodel.train()\n\nepoch_cnt = 0;\n\nfor epoch in range(num_epochs):\n\t#Load in the data in batches using the train_loader object\n    epoch_loss = []\n    running_loss=0\n    correct=0\n    total=0\n    epoch_cnt +=1\n    for i, (images, labels) in enumerate(train_loader):  \n        # Move tensors to the configured device\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        # Forward pass\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        # Backward and optimize\n        \n        loss.backward()\n        epoch_loss.append(loss.item())\n        optimizer.step()\n        \n        running_loss +=loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n    train_loss=running_loss/len(train_loader)\n    accu=100.*correct/total\n    train_accu.append(accu)\n    train_losses.append(train_loss)\n    \n    \n    \n    # Early stopping\n    the_current_loss = validation(model, device, val_loader, criterion)\n    print('The current loss:', the_current_loss)\n\n    if the_current_loss > the_last_loss:\n        trigger_times += 1\n        print('trigger times:', trigger_times)\n\n        if trigger_times >= patience:\n            print('Early stopping!\\nStart to test process.')\n            break\n            \n    else:\n        print('trigger times: 0')\n        trigger_times = 0\n\n    the_last_loss = the_current_loss\n   \n    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:04:25.756034Z","iopub.execute_input":"2022-04-29T19:04:25.756751Z","iopub.status.idle":"2022-04-29T19:08:35.442055Z","shell.execute_reply.started":"2022-04-29T19:04:25.756711Z","shell.execute_reply":"2022-04-29T19:08:35.441252Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"\nplt.plot(np.linspace(1, epoch_cnt, epoch_cnt).astype(int), train_losses)\nplt.legend('Loss', loc='upper right');\n    \nplt.savefig('TrainLoss.png')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:08:35.443736Z","iopub.execute_input":"2022-04-29T19:08:35.443987Z","iopub.status.idle":"2022-04-29T19:08:35.695596Z","shell.execute_reply.started":"2022-04-29T19:08:35.443951Z","shell.execute_reply":"2022-04-29T19:08:35.694925Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.linspace(1, epoch_cnt,epoch_cnt).astype(int), train_accu)\nplt.legend('Accuracy', loc='upper right');\nplt.savefig('TrainAcc.png')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:08:35.696670Z","iopub.execute_input":"2022-04-29T19:08:35.698173Z","iopub.status.idle":"2022-04-29T19:08:35.933543Z","shell.execute_reply.started":"2022-04-29T19:08:35.698130Z","shell.execute_reply":"2022-04-29T19:08:35.932875Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.linspace(1, epoch_cnt,epoch_cnt).astype(int), test_losses)\nplt.legend('Loss', loc='upper right');\n    \nplt.savefig('TestLoss.png')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:08:35.935269Z","iopub.execute_input":"2022-04-29T19:08:35.935871Z","iopub.status.idle":"2022-04-29T19:08:36.183713Z","shell.execute_reply.started":"2022-04-29T19:08:35.935832Z","shell.execute_reply":"2022-04-29T19:08:36.183075Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"code","source":"plt.plot(np.linspace(1, epoch_cnt,epoch_cnt).astype(int), test_accu)\nplt.legend('Accuracy', loc='upper right');\nplt.savefig('TestAcc.png')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:08:36.184749Z","iopub.execute_input":"2022-04-29T19:08:36.185160Z","iopub.status.idle":"2022-04-29T19:08:36.454290Z","shell.execute_reply.started":"2022-04-29T19:08:36.185123Z","shell.execute_reply":"2022-04-29T19:08:36.453632Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"# model.eval()\n# with torch.no_grad():\n#     correct = 0\n#     total = 0\n#     for images, labels in train_loader:\n#         images = images.to(device)\n#         labels = labels.to(device)\n#         outputs = model(images)\n#         _, predicted = torch.max(outputs.data, 1)\n#         total += labels.size(0)\n#         correct += (predicted == labels).sum().item()\n    \n#     print('Accuracy of the network on the {} train images: {} %'.format(50000, 100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:08:36.455475Z","iopub.execute_input":"2022-04-29T19:08:36.455702Z","iopub.status.idle":"2022-04-29T19:08:36.460641Z","shell.execute_reply.started":"2022-04-29T19:08:36.455669Z","shell.execute_reply":"2022-04-29T19:08:36.459805Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"model = joblib.load('filename.pkl')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:08:36.461676Z","iopub.execute_input":"2022-04-29T19:08:36.462733Z","iopub.status.idle":"2022-04-29T19:08:36.514036Z","shell.execute_reply.started":"2022-04-29T19:08:36.462692Z","shell.execute_reply":"2022-04-29T19:08:36.513242Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"model.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for images, labels in test_loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    \n    print('Accuracy of the network on the {} test images: {} %'.format(10000, 100 * correct / total))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T19:08:36.515172Z","iopub.execute_input":"2022-04-29T19:08:36.515430Z","iopub.status.idle":"2022-04-29T19:08:37.947580Z","shell.execute_reply.started":"2022-04-29T19:08:36.515396Z","shell.execute_reply":"2022-04-29T19:08:37.946802Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}